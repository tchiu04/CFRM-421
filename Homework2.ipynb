{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_style": "center",
        "id": "ALsGqUrdsxdK"
      },
      "source": [
        "<h1>\n",
        "<center>CFRM 421/521, Spring 2023</center>\n",
        "</h1>\n",
        "\n",
        "<h1>\n",
        "<center>Terence Chiu</center>\n",
        "</h1>\n",
        "\n",
        "<h1>\n",
        "<center>Homework 2</center>\n",
        "</h1>\n",
        "\n",
        "* **Due: Tuesday, April 29, 2024, 11:59 PM**\n",
        "\n",
        "\n",
        "* Total marks: 45\n",
        "\n",
        "\n",
        "* Late submissions are allowed, but a 20% penalty per day applies. Your last submission is considered for calculating the penalty.\n",
        "\n",
        "\n",
        "*  Use this Jupyter notebook as a template for your solutions. **Your solution must be submitted as both one Jupyter notebook and one PDF file on Gradescope.** There will be two modules on Gradescope, one for each file type. The notebook must be already run, that is, make sure that you have run all the code, save the notebook, and then when you reopen the notebook, checked that all output appears as expected. You are allowed to use code from the textbook, textbook website, or lecture notes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Oz0YdW_sxdL"
      },
      "source": [
        "# 1. Random forest for time series data [14 marks]\n",
        "\n",
        "In this question you will work with the NYSE dataset. Only 3 time series in this dataset will be use: `DJ_return` ($a_t$), `log_volatility` ($b_t$), and `log_volume` ($c_t$). Download the data as a csv file from [Canvas](https://canvas.uw.edu/files/105167110/download?download_frd=1). The data was originally obtained from the R library ISLR2, and you can read the documentation for the dataset [here](https://cran.rstudio.com/web/packages/ISLR2/ISLR2.pdf), which explains the meaning of the variables.\n",
        "\n",
        "You want to predict the 1-step ahead value of `log_volume` $c_{t+1}$ using the previous values of this variable and the other two variables (`DJ_return` and `log_volatility`) up to 5 lags. So the features are $c_{t},\\dots,c_{t-4},b_{t},\\dots,b_{t-4},a_{t},\\dots,a_{t-4}$.\n",
        "\n",
        "If the data is stored in a file named `NYSE.csv` in your working directory, then loading the data can be done using the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qYGPL1w8sxdL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"NYSE.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQy2ky8ysxdM"
      },
      "source": [
        "## (a) [3 marks]\n",
        "\n",
        "Create the feature matrix `X` and the target variable `y`. Print at least the first 2 rows of `X` and `y` (it is acceptable that not every element of the rows are printed)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9gYxNJEsxdM"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib as mlp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_volume = data[\"log_volume\"]\n",
        "DJ_return = data[\"DJ_return\"]\n",
        "log_volatility = data[\"log_volatility\"]\n"
      ],
      "metadata": {
        "id": "c-AJng72AOxF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ts_split(ts, feature_steps=5, target_steps=1):\n",
        "    n_obs = len(ts) - feature_steps - target_steps + 1\n",
        "    X = np.array([ts[idx:idx + feature_steps] for idx in range(n_obs)])\n",
        "    y = np.array([ts[idx + feature_steps:idx + feature_steps + target_steps]\n",
        "                  for idx in range(n_obs)])\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "WiC0N05AEZMm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_log_volume, y_log_volume = ts_split(log_volume)\n",
        "x_DJ_return, y_DJ_return = ts_split(DJ_return)\n",
        "x_log_volatility, y_log_volatility = ts_split(log_volatility)"
      ],
      "metadata": {
        "id": "XesTvCeGEaEh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.c_[x_log_volume, x_DJ_return, x_log_volatility]\n",
        "y = y_log_volume\n",
        "print(\"First 2 rows of X\\n\", X[:2])\n",
        "print(\"First 2 rows of y\\n\", y[:2])"
      ],
      "metadata": {
        "id": "HBed4fQjEk42",
        "outputId": "ee9a660e-74fd-4dcb-e59a-1324232cefc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 2 rows of X\n",
            " [[ 3.25730000e-02  3.46202000e-01  5.25306000e-01  2.10182000e-01\n",
            "   4.41870000e-02 -4.46100000e-03  7.81300000e-03  3.84500000e-03\n",
            "  -3.46200000e-03  5.68000000e-04 -1.31274026e+01 -1.17493047e+01\n",
            "  -1.16656090e+01 -1.16267724e+01 -1.17281302e+01]\n",
            " [ 3.46202000e-01  5.25306000e-01  2.10182000e-01  4.41870000e-02\n",
            "   1.33246000e-01  7.81300000e-03  3.84500000e-03 -3.46200000e-03\n",
            "   5.68000000e-04 -1.08240000e-02 -1.17493047e+01 -1.16656090e+01\n",
            "  -1.16267724e+01 -1.17281302e+01 -1.08725263e+01]]\n",
            "First 2 rows of y\n",
            " [[ 0.133246]\n",
            " [-0.011528]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLIc6RrgsxdM"
      },
      "source": [
        "## (b)  [5 marks]\n",
        "\n",
        "Consider fitting a random forest to predict the 1-step ahead value of `log_volume`. The random forest must include the argument `random_state=42`, and it is useful to also include `n_jobs=-1` (you can use `n_job=-1` throughout this homework wherever it is avaliable). Use 3-fold time series CV split, with the test set split 50% into a validation set and 50% into the actual test set, to tune the hyperparameters `n_estimators` taking the values  200, 400, 600, and the cost-complexity pruning parameter $\\alpha$ taking the values $10^{-k}$, $k=1,3,5,7$. When tuning hyperparameters on the validation sets, fit the model only on a random 10% sample of the instances of the training set on the same CV fold to reduce computational time (that is, use the same reduced training set for all the hyperparameters, but a different one for each CV fold). Note this will still preserve the correct time ordering, and the reduce training set should not be used when fitting and evaluating the best model on the test set. The performance measure is RMSE. Report the best hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qKKUnSSsxdM"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "n_estimators_list = [200, 400, 600]\n",
        "ccp_alpha_list = [10**-1, 10**-3, 10**-5, 10**-7]\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "len_series = log_volume.size"
      ],
      "metadata": {
        "id": "E_EBAI0BEvc9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def time_series_valid_test(X, y, n_split, valid_or_test, optimal_par=None):\n",
        "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
        "    rf_rmse = []\n",
        "    current_rmse = []\n",
        "    i = 0\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        i += 1\n",
        "        # Break test set into 50% validation set, 50% test set\n",
        "        break_test_ind = int(test_index[0] + 0.5*(test_index[-1]-test_index[0]))\n",
        "        valid_index = np.array(list(range(test_index[0],break_test_ind)))\n",
        "        test_index = np.array(list(range(break_test_ind,test_index[-1])))\n",
        "\n",
        "        # Split\n",
        "        X_train, X_valid, X_test = X[train_index], X[valid_index], X[test_index]\n",
        "        y_train, y_valid, y_test = y[train_index], y[valid_index], y[test_index]\n",
        "\n",
        "        # Tuning\n",
        "        if valid_or_test == \"valid\":\n",
        "            X_train_reduced, X_train_rest, y_train_reduced, y_test_reduced = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "            for alpha in ccp_alpha_list:\n",
        "                for n_estimators in n_estimators_list:\n",
        "                    model_rf = RandomForestRegressor(random_state=42, n_jobs=-1,\n",
        "                               ccp_alpha=alpha, n_estimators=n_estimators)\n",
        "                    model_rf.fit(X_train_reduced, y_train_reduced.ravel())\n",
        "                    y_val_rf = model_rf.predict(X_valid)\n",
        "                    rf_rmse.append(np.sqrt(mean_squared_error(y_valid, y_val_rf)))\n",
        "\n",
        "        # Evalulate on test set\n",
        "        if valid_or_test == \"test\":\n",
        "            model_rf = RandomForestRegressor(random_state=42, n_jobs=-1,\n",
        "                       ccp_alpha=optimal_par[0], n_estimators=optimal_par[1])\n",
        "            model_rf.fit(X_train, y_train.ravel())\n",
        "            y_test_rf = model_rf.predict(X_test)\n",
        "            rf_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_rf)))\n",
        "\n",
        "            # use current value to predict next value\n",
        "            y_test_current = y[test_index - 1]\n",
        "            current_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_current)))\n",
        "\n",
        "            # Plot the prediction for the last CV fold\n",
        "            if i == n_split:\n",
        "                plt.plot(range(len_series - test_index.size, len_series),\n",
        "                         y_test_rf, label=\"1-step ahead prediction\")\n",
        "                plt.plot(range(len_series - test_index.size,len_series),\n",
        "                         y_test, \"--\", alpha = 0.6, label=\"True value\")\n",
        "                plt.legend(loc=\"upper left\")\n",
        "                plt.show()\n",
        "\n",
        "    # Average RMSE over CV folds\n",
        "    if valid_or_test == \"valid\":\n",
        "        rf_rmse = np.mean(np.array(rf_rmse).reshape(\n",
        "            n_split, len(ccp_alpha_list)*len(n_estimators_list)), axis=0)\n",
        "        return rf_rmse\n",
        "\n",
        "    if valid_or_test == \"test\":\n",
        "        rf_rmse = np.mean(rf_rmse)\n",
        "        current_rmse = np.mean(current_rmse)\n",
        "        return rf_rmse, current_rmse, model_rf"
      ],
      "metadata": {
        "id": "G_RYEYWdKykM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_rmse = time_series_valid_test(X, y, 3,\"valid\")\n",
        "print(rf_rmse)"
      ],
      "metadata": {
        "id": "TcAfxb4QK9Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpTsxAPMsxdM"
      },
      "source": [
        "## (c)  [2 marks]\n",
        "\n",
        "Using the same time series split as in (b), compute the RMSE of the best fitting model on the test set, and include a plot of the true values and predicted values on the test set of the last fold (the fold closest to the current time) of the CV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQkOiZXRsxdM"
      },
      "source": [
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EKbk71PKTuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEx68ab8sxdM"
      },
      "source": [
        "## (d) [2 marks]\n",
        "\n",
        "It is often useful to check that your model is not worse than a very simple method of prediction. On the test set, compute the RMSE of a model that simply predicts the 1-step ahead value of `log_volume` $c_{t+1}$ as the current value $c_t$, and compare this to the best fitting random forest model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQcyVNQ8sxdM"
      },
      "source": [
        "**Solution**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbi2royhsxdM"
      },
      "source": [
        "## (e) [2 marks]\n",
        "\n",
        "Compute the feature importances of the best fitting model. Which feature is the most important and what is its feature importance value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFDabWJzsxdM"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6qo6A5HsxdM"
      },
      "source": [
        "# 2. SVM classification and regression [11 marks]\n",
        "\n",
        "For all SVM models in this question use a standard scaler.\n",
        "\n",
        "## (a) [2 marks]\n",
        "\n",
        "In this question, a SVM is used for classification for the MNIST dataset. The following code loads the MNIST dataset, creates the test set, and to reduce training time, takes a random sample of 2000 points from the full training set to use as your actual training set stored in `X` and `y`. Do not shuffle the data.\n",
        "\n",
        "Hint: Reading the solution to Question 9 in the Chapter 5 [Jupyter notebook](https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb) (2nd edition) on the textbook website may help with this question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19qh9BYFsxdM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as mlp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', as_frame=False, cache=True, parser='auto')\n",
        "X_train = mnist[\"data\"][:60000]\n",
        "X_test  = mnist[\"data\"][60000:]\n",
        "y_train = mnist[\"target\"][:60000]\n",
        "y_test  = mnist[\"target\"][60000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y3Qhx6msxdM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "N = 2000\n",
        "split_obj = StratifiedShuffleSplit(n_splits=1,\n",
        "                               test_size=N/60000, random_state=42)\n",
        "for other_idx, subsample_idx in split_obj.split(X_train, y_train):\n",
        "    X = X_train[subsample_idx]\n",
        "    y = y_train[subsample_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOG-67ASsxdM"
      },
      "source": [
        "**Task:** Consider fitting the linear SVM classifier (`LinearSVC`) with `max_iter=50000`. For this model, optimize the hyperparameter $C$ using 3-fold CV over the values $10^{-k}$, $k=0,1,\\dots,9$, where the performance measure is accuracy. What is the best $C$ and what is the accuracy in this case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yemxcmtsxdM"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QvWvcw4sxdN"
      },
      "source": [
        "## (b) [2 marks]\n",
        "\n",
        "**Task:** Now consider fitting a SVM with a Gaussian RBF kernel and `max_iter=50000`. For this model, optimize the hyperparameters $C$ over the distrbution `uniform(1,10)` and $\\gamma$ over the distribution `loguniform(0.0001, 0.1)` from `scipy.stats.loguniform` with 10 random samples. The `loguniform(a,b)` function takes a random sample from the probability distribution with pdf $f(x)\\propto 1/x, x\\in[a,b]$. Again, use 3-fold CV and the performance measure is accuracy. What are the best hyperparameters and what is the accuracy in this case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiOEyLPisxdN"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prSDnF0RsxdN"
      },
      "source": [
        "## (c) [2 mark]\n",
        "\n",
        "**Task:** Choose the best model in (a) and (b). Then for this model, evaluate the accuracy on the test set, which is stored in `X_test` and `y_test`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXEcM8x4sxdN"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTq3h5W5sxdN"
      },
      "source": [
        "## (d) [3 marks]\n",
        "\n",
        "Consider the original source of the California housing data (which is different from the modified dataset used in Homework 1) in Scikit-Learn. The data is obtained and split using the code below. The training set is stored in `X_train` and `y_train`. Do not shuffle the data.\n",
        "\n",
        "Hint: Reading the solution to Question 11 in the Chapter 5 [Jupyter notebook](https://github.com/ageron/handson-ml3/blob/main/05_support_vector_machines.ipynb) (3rd edition) on the textbook website may help with this question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOCQSN90sxdN"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWphVed6sxdN"
      },
      "source": [
        "**Task:** Consider SVM regression with a Gaussian RBF kernel and a sigmoid kernel with `max_iter=50000`. For both models, use randomized search to choose good hyperparameter values for `C` and `gamma`, and set the arguement `random_state=42`. For both models, optimize the hyperparameters $C$ over the distrbution `uniform(1,20)` and $\\gamma$ over the distribution `loguniform(0.0001, 0.1)` with 20 random samples. To save training time, use only the first 2000 instances of `X_train` and `y_train` (which have been randomly shuffled already) for the search. Again, use 3-fold CV and the performance measure is MSE. What are the best hyperparameters and what is the MSE in this case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxufcVW4sxdN"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBeYvZdJsxdN"
      },
      "source": [
        "## (e) [2 marks]\n",
        "\n",
        "**Task:** Choose the best model in (d). But now refit it on the full training set (not just the first 2000 instances). Then for this model, evaluate the RMSE on the test set, which is stored in `X_test` and `y_test`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDNH4XFJsxdN"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tjZiAA4sxdN"
      },
      "source": [
        "# 3. Voting classifiers [11 marks]\n",
        "## (a)  [4 marks]\n",
        "\n",
        "Consider the MNIST dataset. To save computational time, after spliting into a training, validation and test set, we keep only the first 5000 instances of the training set, and only the first 1000 instances of the validation and test set, as given by the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYxADWNAsxdN"
      },
      "outputs": [],
      "source": [
        "N = 50_000\n",
        "M = 60_000\n",
        "X_train = mnist[\"data\"][:N][:5000]\n",
        "y_train = mnist[\"target\"][:N][:5000]\n",
        "X_valid  = mnist[\"data\"][N:M][:1000]\n",
        "y_valid = mnist[\"target\"][N:M][:1000]\n",
        "X_test  = mnist[\"data\"][M:][:1000]\n",
        "y_test = mnist[\"target\"][M:][:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Mc4aOJsxdN"
      },
      "source": [
        "Do not shuffle the data and do not use a standard scaler. Train the following classifiers on the training set:\n",
        "\n",
        "(i) a multilayer perceptron classifier using the class `MLPClassifier()` from `sklearn.neural_network` with arguments `random_state=42`,\n",
        "\n",
        "(ii) an extra-trees classifier with arguments `n_estimators=100, n_jobs=-1, random_state=42`,\n",
        "\n",
        "(iii) an AdaBoost classifier  with arguments  `n_estimators=50, learning_rate=0.2, random_state=42`,\n",
        "\n",
        "(iv) a gradient boosting classifier using the class `GradientBoostingClassifier()` with arguments `max_depth=2, n_estimators=10, learning_rate=0.25, random_state=42`.\n",
        "\n",
        "Report the accuracy of each trained classifier on the validation set.\n",
        "\n",
        "Hint: Reading the solution to Question 8 in the Chapter 7 [Jupyter notebook](https://github.com/ageron/handson-ml3/blob/main/07_ensemble_learning_and_random_forests.ipynb) on the textbook website may help with this question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8qY5kJFsxdN"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-g9YEkSsxdN"
      },
      "source": [
        "## (b)  [5 marks]\n",
        "\n",
        "Train the following models:\n",
        "\n",
        "* a hard-voting ensemble classifier for all the models in (a)\n",
        "* a soft-voting ensemble classifier for all the models in (a)\n",
        "* a hard-voting ensemble classifier dropping the worst performing model in (a)\n",
        "* a soft-voting ensemble classifier dropping the worst performing model in (a)\n",
        "\n",
        "Evaluate the accuracy of these voting classifiers on the validation set, and compare it to the performance of the individual models in (a)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1s-HEXmsxdN"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEWV7atOsxdN"
      },
      "source": [
        "## (c)  [2 marks]\n",
        "\n",
        "Of the four voting classifiers in (b), choose the best model. Then evaluate the accuracy of this model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy0da4RZsxdN"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKyHhKRIsxdO"
      },
      "source": [
        "# 4. Stacking [9 marks]\n",
        "\n",
        "We continue with the setting of Question 3. The training set, validation set and test set are the same. In Question 3, we have used predetermined rules (that is, hard-voting and soft-voting) to build the ensemble prediction. **Stacking** is an ensemble method in which you train a model (called a **blender**) to aggregate the result of each predictor into an ensemble prediction.\n",
        "\n",
        "Hint: Reading the subsection \"Stacking\" in Chapter 7 of the textbook and the solution to Question 9 in the Chapter 7 [Jupyter notebook](https://github.com/ageron/handson-ml3/blob/main/07_ensemble_learning_and_random_forests.ipynb) on the textbook website may help with this question.\n",
        "\n",
        "## (a)  [3 marks]\n",
        "\n",
        "For each of the four classifiers in Question 3(a), make 5000 clean predictions on the training set with 3-fold cross validation using `sklearn.model_selection.cross_val_predict()`. You should end up with four predictions per observation. Print at least the first 5 rows of `pred`. Next, apply one-hot encoding to `pred` since these predictions are class labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNEfscYzsxdO"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjR0Z4zysxdO"
      },
      "source": [
        "## (b) [3 marks]\n",
        "Use the predictions in (a) as features and the actual label of the observations as the target. Train a random forest classifier on the training set with the parameters `n_estimators=100, random_state=42`.  This classifier is a blender."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyAePKTzsxdS"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJeM6OUQsxdS"
      },
      "source": [
        "## (c) [3 marks]\n",
        "\n",
        "Obtain the predictions of the blender on the test set by feeding predictions on the test set from the four classifiers in Question 3(a) into the blender trained in Question 4(b). Do not retrain the blender. These are called stacking predictions. Report the accuracy of your stacking predictions on the test set and compare this to the results in Question 3(c)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9CAIeQPsxdS"
      },
      "source": [
        "**Solution:**"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": false,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}